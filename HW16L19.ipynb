{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Reviews SpaCy Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a simple NLP model on IMDB Dataset of 50K Movie Reviews. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It consists of a set of 25,000 highly polar movie reviews for training and 25,000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./IMDB Dataset.csv', \n",
    "                 encoding='utf-8')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there HTML marks and other peculiarities in the text. Let's remove HTML tags isung RegEx and lemmatize the text using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>reviewer mention watch 1 oz episode hook right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure like resurrection date seahunt series tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>amazing fresh innovative idea 70 air 7 8 year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>encourage positive comment film look forward w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>like original gut wrench laughter like movie y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "5  Probably my all-time favorite movie, a story o...  positive   \n",
       "6  I sure would like to see a resurrection of a u...  positive   \n",
       "7  This show was an amazing, fresh & innovative i...  negative   \n",
       "8  Encouraged by the positive comments about this...  negative   \n",
       "9  If you like original gut wrenching laughter yo...  positive   \n",
       "\n",
       "                                    review_processed  \n",
       "0  reviewer mention watch 1 oz episode hook right...  \n",
       "1  wonderful little production film technique una...  \n",
       "2  think wonderful way spend time hot summer week...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stunnin...  \n",
       "5  probably time favorite movie story selflessnes...  \n",
       "6  sure like resurrection date seahunt series tec...  \n",
       "7  amazing fresh innovative idea 70 air 7 8 year ...  \n",
       "8  encourage positive comment film look forward w...  \n",
       "9  like original gut wrench laughter like movie y...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.like_url and not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['review_processed'] = df['review'].apply(preprocess_text)            #df['review'].head(10).apply(preprocess_text) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 50-50 train-test split as in the dataset description ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_processed'].values, df['sentiment'].values, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a pipeline with TF IDF and Support Vector Machine classifier. In the cross-validation part, I tune the tokenizer, unigrams or unigrams and bigrams as well as SVM hyperparameters such as regularization term C and the kernel (linear or Radial Basis Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 5/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.894, test=0.861) total time= 8.4min\n",
      "[CV 4/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.894, test=0.861) total time= 8.4min\n",
      "[CV 3/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.893, test=0.858) total time= 8.4min\n",
      "[CV 2/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.891, test=0.865) total time= 8.4min\n",
      "[CV 1/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.889, test=0.882) total time= 8.5min\n",
      "[CV 3/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.875, test=0.842) total time=16.4min\n",
      "[CV 2/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.875, test=0.847) total time=16.4min\n",
      "[CV 1/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.873, test=0.857) total time=16.5min\n",
      "[CV 3/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.852, test=0.807) total time=11.4min\n",
      "[CV 1/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.848, test=0.821) total time=11.5min\n",
      "[CV 2/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.852, test=0.817) total time=11.5min\n",
      "[CV 5/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.878, test=0.840) total time=18.1min\n",
      "[CV 4/5] END clf__C=0.1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.878, test=0.840) total time=18.1min\n",
      "[CV 5/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.855, test=0.813) total time=10.9min\n",
      "[CV 4/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.857, test=0.811) total time=10.9min\n",
      "[CV 1/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.962, test=0.902) total time= 8.5min\n",
      "[CV 1/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.736, test=0.732) total time=17.1min\n",
      "[CV 2/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.965, test=0.887) total time= 8.8min\n",
      "[CV 3/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.964, test=0.882) total time= 8.9min\n",
      "[CV 3/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.748, test=0.733) total time=15.8min\n",
      "[CV 4/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.752, test=0.737) total time=15.8min\n",
      "[CV 2/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.745, test=0.728) total time=15.9min\n",
      "[CV 4/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.964, test=0.881) total time= 8.1min\n",
      "[CV 5/5] END clf__C=0.1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.749, test=0.730) total time=14.8min\n",
      "[CV 5/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.963, test=0.890) total time= 7.9min\n",
      "[CV 1/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.992, test=0.904) total time=17.8min\n",
      "[CV 1/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.996, test=0.908) total time=23.6min\n",
      "[CV 2/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.996, test=0.895) total time=23.7min\n",
      "[CV 2/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.991, test=0.890) total time=19.4min\n",
      "[CV 3/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.992, test=0.883) total time=19.8min\n",
      "[CV 4/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.996, test=0.888) total time=25.6min\n",
      "[CV 5/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.996, test=0.894) total time=25.5min\n",
      "[CV 3/5] END clf__C=1, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=0.996, test=0.890) total time=25.7min\n",
      "[CV 4/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.993, test=0.882) total time=26.3min\n",
      "[CV 5/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=0.992, test=0.894) total time=26.3min\n",
      "[CV 1/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.999, test=0.873) total time=27.5min\n",
      "[CV 1/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.999, test=0.893) total time=42.4min\n",
      "[CV 2/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.998, test=0.883) total time=42.8min\n",
      "[CV 3/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.999, test=0.877) total time=42.9min\n",
      "[CV 4/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.999, test=0.882) total time=42.5min\n",
      "[CV 5/5] END clf__C=1, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=0.999, test=0.879) total time=42.6min\n",
      "[CV 2/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.999, test=0.862) total time=24.9min\n",
      "[CV 3/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.859) total time=24.3min\n",
      "[CV 4/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.867) total time=22.7min\n",
      "[CV 5/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 1);, score=(train=0.999, test=0.867) total time=17.9min\n",
      "[CV 1/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.906) total time=18.8min\n",
      "[CV 2/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.893) total time=18.6min\n",
      "[CV 1/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.908) total time=29.2min\n",
      "[CV 2/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.894) total time=29.0min\n",
      "[CV 4/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.888) total time=28.5min\n",
      "[CV 3/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.892) total time=28.8min\n",
      "[CV 5/5] END clf__C=10, clf__kernel=linear, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.895) total time=28.6min\n",
      "[CV 3/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.886) total time=18.8min\n",
      "[CV 4/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.885) total time=18.6min\n",
      "[CV 5/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 1);, score=(train=1.000, test=0.896) total time=18.5min\n",
      "[CV 1/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.900) total time=28.8min\n",
      "[CV 2/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.886) total time=28.5min\n",
      "[CV 3/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.882) total time=28.3min\n",
      "[CV 4/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.885) total time=28.1min\n",
      "[CV 5/5] END clf__C=10, clf__kernel=rbf, vect__ngram_range=(1, 2);, score=(train=1.000, test=0.883) total time=27.9min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;clf&#x27;, SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [0.1, 1, 10],\n",
       "                          &#x27;clf__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1), (1, 2)]}],\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;clf&#x27;, SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [0.1, 1, 10],\n",
       "                          &#x27;clf__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1), (1, 2)]}],\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [0.1, 1, 10],\n",
       "                          'clf__kernel': ['linear', 'rbf'],\n",
       "                          'vect__ngram_range': [(1, 1), (1, 2)]}],\n",
       "             return_train_score=True, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_param= [\n",
    "            {'clf__C': [0.1, 1, 10],\n",
    "             'clf__kernel': ['linear', 'rbf'],\n",
    "             'vect__ngram_range': [(1, 1), (1, 2)]}\n",
    "           ]\n",
    "\n",
    "svm_tfidf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(norm='l2')),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "gs_svm_tfidf = GridSearchCV(svm_tfidf, svm_param,\n",
    "                            scoring='accuracy',\n",
    "                            cv=5,\n",
    "                            verbose=3,\n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "gs_svm_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the best parameter set and its CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set for SVM: {'clf__C': 10, 'clf__kernel': 'linear', 'vect__ngram_range': (1, 2)}\n",
      "Cross-validation Accuracy of SVM: 0.8955\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set for SVM: {gs_svm_tfidf.best_params_}')\n",
    "print(f'Cross-validation Accuracy of SVM: {gs_svm_tfidf.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best model uses bigrams. The linear kernel of SVM shows no much of nonlinearity in this classification problem. Let's show the best estimator's test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of tuned SVM: 0.8937\n"
     ]
    }
   ],
   "source": [
    "clf_svm = gs_svm_tfidf.best_estimator_\n",
    "print(f'Test Accuracy of tuned SVM: {clf_svm.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty good test accuracy for a simple SVM model on IMDB dataset. The test accuracy for spaCy sentiment analysis data is almost the same as for the previous IMDB project with RegEx and NLTK manual pre-processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
